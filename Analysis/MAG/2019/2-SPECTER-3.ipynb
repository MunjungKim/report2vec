{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data02/MAG_PROCESSED_DATA/2019FILE/TITLE_SEP_ABSTRACT.txt.3\", \"r\") as fp:\n",
    "    paper_texts = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2155965"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paper_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('allenai-specter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.18655"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2155965/300000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# corpus_embeddings = model.encode(paper_texts[:10000], convert_to_tensor=True)\n",
    "# embeddings = corpus_embeddings.cpu().numpy()\n",
    "# np.save('/data02/MAG_PROCESSED_DATA/SPECTER_VECTOR/sBERT_embedding.npy.0',embeddings)\n",
    "# del embeddings\n",
    "# del corpus_embeddings\n",
    "# print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU  | MEM |\n",
      "-------------------\n",
      "|  0 | 100% | 31% |\n",
      "|  1 |   0% |  9% |\n",
      "|  2 | 100% | 26% |\n",
      "|  3 |   0% | 31% |\n"
     ]
    }
   ],
   "source": [
    "GPUtil.showUtilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(paper_texts_temp):\n",
    "    corpus_embeddings = model.encode(paper_texts_temp, convert_to_tensor=True)\n",
    "    embeddings = corpus_embeddings.cpu().numpy()\n",
    "    \n",
    "    del corpus_embeddings\n",
    "    torch.cuda.empty_cache()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET lowest gpu memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, re, os, sys\n",
    "\n",
    "# GPU picking\n",
    "# http://stackoverflow.com/a/41638727/419116\n",
    "# Nvidia-smi GPU memory parsing.\n",
    "# Tested on nvidia-smi 370.23\n",
    "\n",
    "def run_command(cmd):\n",
    "    \"\"\"Run command, return output as string.\"\"\"\n",
    "    \n",
    "    output = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True).communicate()[0]\n",
    "    return output.decode(\"ascii\")\n",
    "\n",
    "def list_available_gpus():\n",
    "    \"\"\"Returns list of available GPU ids.\"\"\"\n",
    "    \n",
    "    output = run_command(\"nvidia-smi -L\")\n",
    "    # lines of the form GPU 0: TITAN X\n",
    "    gpu_regex = re.compile(r\"GPU (?P<gpu_id>\\d+):\")\n",
    "    result = []\n",
    "    for line in output.strip().split(\"\\n\"):\n",
    "        m = gpu_regex.match(line)\n",
    "        assert m, \"Couldnt parse \"+line\n",
    "        result.append(int(m.group(\"gpu_id\")))\n",
    "    return result\n",
    "\n",
    "def gpu_memory_map():\n",
    "    \"\"\"Returns map of GPU id to memory allocated on that GPU.\"\"\"\n",
    "\n",
    "    output = run_command(\"nvidia-smi\")\n",
    "    gpu_output = output[output.find(\"GPU Memory\"):]\n",
    "    # lines of the form\n",
    "    # |    0      8734    C   python                                       11705MiB |\n",
    "    memory_regex = re.compile(r\"[|]\\s+?(?P<gpu_id>\\d+)\\D+?(?P<pid>\\d+).+[ ](?P<gpu_memory>\\d+)MiB\")\n",
    "    rows = gpu_output.split(\"\\n\")\n",
    "    result = {gpu_id: 0 for gpu_id in list_available_gpus()}\n",
    "    for row in gpu_output.split(\"\\n\"):\n",
    "        m = memory_regex.search(row)\n",
    "        if not m:\n",
    "            continue\n",
    "        gpu_id = int(m.group(\"gpu_id\"))\n",
    "        gpu_memory = int(m.group(\"gpu_memory\"))\n",
    "        result[gpu_id] += gpu_memory\n",
    "    return result\n",
    "\n",
    "def pick_gpu_lowest_memory():\n",
    "    \"\"\"Returns GPU with the least allocated memory\"\"\"\n",
    "\n",
    "    memory_gpu_map = [(memory, gpu_id) for (gpu_id, memory) in gpu_memory_map().items()]\n",
    "    best_memory, best_gpu = sorted(memory_gpu_map)[0]\n",
    "    return best_gpu\n",
    "\n",
    "def setup_one_gpu():\n",
    "    assert not 'tensorflow' in sys.modules, \"GPU setup must happen before importing TensorFlow\"\n",
    "    gpu_id = pick_gpu_lowest_memory()\n",
    "    print(\"Picking GPU \"+str(gpu_id))\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "\n",
    "def setup_no_gpu():\n",
    "    if 'tensorflow' in sys.modules:\n",
    "        print(\"Warning, GPU setup must happen before importing TensorFlow\")\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pick_gpu_lowest_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 3355, 1: 1013, 2: 2813, 3: 3455}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_memory_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picking GPU 1\n"
     ]
    }
   ],
   "source": [
    "setup_one_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [49:09<5:44:04, 2949.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU  | MEM |\n",
      "-------------------\n",
      "|  0 | 100% | 30% |\n",
      "|  1 |   0% | 24% |\n",
      "|  2 | 100% | 26% |\n",
      "|  3 |   0% | 31% |\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [1:38:27<4:55:11, 2951.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU  | MEM |\n",
      "-------------------\n",
      "|  0 | 100% | 30% |\n",
      "|  1 |   7% | 24% |\n",
      "|  2 | 100% | 26% |\n",
      "|  3 |   0% | 31% |\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [2:27:48<4:06:13, 2954.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU  | MEM |\n",
      "-------------------\n",
      "|  0 | 100% | 30% |\n",
      "|  1 |   0% | 24% |\n",
      "|  2 |  99% | 34% |\n",
      "|  3 |   0% | 31% |\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for n in tqdm(range(8)):\n",
    "    \n",
    "\n",
    "    print(n)\n",
    "    embeddings = get_vectors(paper_texts[n*300000: (n+1)*300000])\n",
    "                  \n",
    "    directory = '/data02/MAG_PROCESSED_DATA/2019FILE/sBERT_vector_2019_' + str(n+20)\n",
    "    \n",
    "    np.save(directory,embeddings)\n",
    "    \n",
    "              \n",
    "    del embeddings\n",
    "    del directory\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    GPUtil.showUtilization()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PID 155694"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-99ad86a61454>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mcorpus_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "del corpus_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
